{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "\n",
    "Perform the data analysis in a “streaming” manner (treat data as streaming). \n",
    "\n",
    "Use Kafka as a message broker and write a custom producer (reading data from files) and a custom consumer\n",
    "(for processing data). \n",
    "\n",
    "Since data is (with few exceptions, most likely errors) ordered by the “summons number” you can assume that the lines in CSV files are in chronological order\n",
    "(important for producing Kafka messages). \n",
    "\n",
    "Decide if using different topics can help you. \n",
    "\n",
    "Show rolling descriptive statistics (mean, standard deviation, …, think of at least three more) for all data, boroughs, and for the 10 most interesting streets (with highest numbers of tickets overall, or by your qualified choice). \n",
    "\n",
    "For the same data, choose, implement, and apply a stream clustering algorithm (preferably spatial clustering) of your choice [7].\n",
    "\n",
    "Note: stream processing is better not performed in the Arnes cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import confluent_kafka as kafka, socket\n",
    "import os, socket, uuid, json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read names of all csv files from datasets/original_data/ to use as input and street location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '../datasets/'\n",
    "\n",
    "csv_files = [f for f in os.listdir(base_path + \"original_data/\") if f.endswith('.csv')]\n",
    "csv_files.sort()\n",
    "\n",
    "street_locations = pd.read_csv(base_path + 'street_locations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_chunk(chunk):\n",
    "    chunk = chunk.dropna(subset=['summons_number'])\n",
    "    chunk = chunk.drop_duplicates()\n",
    "\n",
    "    chunk['street_code'] = chunk['street_code1'].where(chunk['street_code1'] != 0, chunk['street_code2'].where(chunk['street_code2'] != 0, chunk['street_code3'])).astype(\"string\")\n",
    "    chunk['street_code'] = chunk['street_code'].replace('0', pd.NA)\n",
    "    chunk.drop(['street_code1', 'street_code2', 'street_code3'], axis=1, inplace=True)\n",
    "    chunk.dropna(subset=['street_code'], inplace=True)\n",
    "    chunk['street_code'] = chunk['street_code'].astype(int)\n",
    "    \n",
    "    chunk['issue_date'] = pd.to_datetime(chunk['issue_date'], format=\"mixed\")\n",
    "    \n",
    "    chunk['violation_time'] = chunk['violation_time'].astype(str)\n",
    "    # add M to the end of the time to read it from 12 hour format\n",
    "    chunk['violation_time'] = chunk[\"violation_time\"].str.upper() + \"M\"\n",
    "    # replace the values that starts with 00 to 12\n",
    "    chunk['violation_time'] = chunk['violation_time'].str.replace(r'^00', '12', regex=True)\n",
    "    # convert the time to 24 hour format\n",
    "    chunk['violation_time'] = pd.to_datetime(chunk[\"violation_time\"], format=\"%I%M%p\", errors=\"coerce\")\n",
    "    # combine the date and time\n",
    "    chunk['issue_date'] = chunk[\"issue_date\"].dt.strftime('%Y-%m-%d') + ' ' + chunk[\"violation_time\"].dt.strftime('%H:%M:%S')\n",
    "    \n",
    "    chunk = chunk.dropna(subset=['issue_date'])\n",
    "    chunk = chunk.drop([\"violation_time\"], axis=1)\n",
    "\n",
    "    #chunk['vehicle_expiration_date'] = pd.to_datetime(chunk['vehicle_expiration_date'], format='%Y%m%d', errors='coerce')\n",
    "\n",
    "    chunk['time_first_observed'] = chunk['time_first_observed'].astype(str)\n",
    "    chunk['time_first_observed'] = chunk[\"time_first_observed\"].str.upper() + \"M\"\n",
    "    chunk['time_first_observed'] = chunk['time_first_observed'].str.replace(r'^00', '12', regex=True)\n",
    "    chunk['time_first_observed'] = pd.to_datetime(chunk['time_first_observed'], format='%I%M%p', errors='coerce')\n",
    "\n",
    "    # replace 0 with NaN\n",
    "    chunk['date_first_observed'] = chunk['date_first_observed'].replace('0', pd.NaT)\n",
    "    # replace 0001-01-03T12:00:00.000 with NaN\n",
    "    chunk['date_first_observed'] = chunk['date_first_observed'].replace('0001-01-03T12:00:00.000', pd.NaT)\n",
    "\n",
    "    chunk['date_first_observed'] = pd.to_datetime(chunk['date_first_observed'], format='%Y%m%d', errors='coerce')\n",
    "\n",
    "    # merge the date and time\n",
    "    chunk['date_first_observed'] = chunk[\"date_first_observed\"].dt.strftime('%Y-%m-%d') + ' ' + chunk[\"time_first_observed\"].dt.strftime('%H:%M:%S')\n",
    "    chunk = chunk.drop([\"time_first_observed\"], axis=1)\n",
    "\n",
    "    # translate the county names to the borough names\n",
    "    county_to_borough = {\n",
    "        \"BRONX\": \"BX\", # Bronx\n",
    "        \"BX\": \"BX\",\n",
    "        \"Bronx\": \"BX\",\n",
    "        \"BRONX\": \"BX\",\n",
    "        \"BK\": \"K\", # Brooklyn known as Kings\n",
    "        \"K\": \"K\",\n",
    "        \"Kings\": \"K\",\n",
    "        \"KINGS\": \"K\",\n",
    "        \"KING\": \"K\",\n",
    "        \"Q\": \"Q\", # Queens\n",
    "        \"QN\": \"Q\",\n",
    "        \"Qns\": \"Q\",\n",
    "        \"QUEEN\": \"Q\",\n",
    "        \"QUEENS\": \"Q\",\n",
    "        \"QNS\": \"Q\",\n",
    "        \"QU\": \"Q\",\n",
    "        \"NY\": \"NY\", # Manhattan known as New York\n",
    "        \"MN\": \"NY\",\n",
    "        \"MAN\": \"NY\",\n",
    "        \"NEW Y\": \"NY\",\n",
    "        \"NEWY\": \"NY\",\n",
    "        \"NYC\": \"NY\",\n",
    "        \"ST\": \"R\", # Staten Island known as Richmond\n",
    "        \"R\": \"R\",\n",
    "        \"Rich\": \"R\",\n",
    "        \"RICH\": \"R\",\n",
    "        \"RICHM\": \"R\",\n",
    "        \"RC\": \"R\",\n",
    "        \"MH\": \"NY\",\n",
    "        \"MS\": \"NY\",\n",
    "        \"N\": \"NY\",\n",
    "        \"P\": \"NY\",\n",
    "        \"PBX\": \"NY\",\n",
    "        \"USA\": \"NY\",\n",
    "        \"VINIS\": \"NY\",\n",
    "        \"A\": pd.NA,\n",
    "        \"F\": pd.NA,\n",
    "        \"ABX\": pd.NA,\n",
    "        \"108\": pd.NA,\n",
    "        \"103\": \"R\", # Staten Island zip code\n",
    "        \"00000\": pd.NA,\n",
    "        \"K   F\": pd.NA,\n",
    "    }\n",
    "\n",
    "    chunk['violation_county'] = chunk['violation_county'].map(county_to_borough)\n",
    "\n",
    "    borough_to_code = {\n",
    "    'NY': 1,\n",
    "    'BX': 2,\n",
    "    'K': 3,\n",
    "    'Q': 4,\n",
    "    'R': 5\n",
    "    }\n",
    "\n",
    "    chunk['violation_county'] = chunk['violation_county'].map(borough_to_code)\n",
    "\n",
    "    # drop the rows that have NaN in the violation_county\n",
    "    chunk = chunk.dropna(subset=['violation_county'])\n",
    "    \n",
    "    # merge on street_code and StreetCode\n",
    "\n",
    "    chunk = chunk.merge(street_locations, how='left', left_on=['street_code', 'violation_county'], right_on=['StreetCode', 'Borough'])\n",
    "    chunk = chunk.dropna(subset=['Latitude', 'Longitude'])\n",
    "\n",
    "    # rename latitude and longitude and drop Street, Borough\n",
    "    chunk = chunk.rename(columns={'Latitude': 'latitude', 'Longitude': 'longitude'})\n",
    "    chunk = chunk.drop(['Street'], axis=1)\n",
    "\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization of kafka producer and consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = kafka.Producer({'bootstrap.servers': \"localhost:29092\",\n",
    "                  'client.id': socket.gethostname()})\n",
    "                  \n",
    "consumer = kafka.Consumer({'bootstrap.servers': \"localhost:29092\",\n",
    "                           'client.id': socket.gethostname(),\n",
    "                           'group.id': 'test_group', \n",
    "                           'auto.offset.reset': 'earliest'})\n",
    "\n",
    "topic = \"parking_violations\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parking Violations\n",
    "To send the info about parking violations we have created a specific class to store the object that will be sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting parking_violations.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile parking_violations.py\n",
    "\n",
    "import faust\n",
    "\n",
    "class ParkingViolation(faust.Record, validation=True):\n",
    "    summons_number: int\n",
    "    plate_id: str\n",
    "    registration_state: str\n",
    "    plate_type: str\n",
    "    violation_code: str\n",
    "    vehicle_body_type: str\n",
    "    vehicle_make: str\n",
    "    issuing_agency: str\n",
    "    street_code: str\n",
    "    vehicle_expiration_date: str\n",
    "    violation_location: str\n",
    "    violation_precinct: str\n",
    "    issuer_precinct: str\n",
    "    issuer_code: str\n",
    "    issuer_command: str\n",
    "    issuer_squad: str\n",
    "    violation_county: str\n",
    "    violation_in_front_of_or_opposite: str\n",
    "    house_number: str\n",
    "    street_name: str\n",
    "    intersecting_street: str\n",
    "    date_first_observed: str\n",
    "    law_section: str\n",
    "    sub_division: str\n",
    "    violation_legal_code: str\n",
    "    days_parking_in_effect: str\n",
    "    from_hours_in_effect: str\n",
    "    to_hours_in_effect: str\n",
    "    vehicle_color: str\n",
    "    unregistered_vehicle: str\n",
    "    vehicle_year: str\n",
    "    meter_number: str\n",
    "    feet_from_curb: str\n",
    "    violation_post_code: str\n",
    "    violation_description: str\n",
    "    no_standing_or_stopping_violation: str\n",
    "    hydrant_violation: str\n",
    "    double_parking_violation: str\n",
    "    latitude: str\n",
    "    longitude: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "To store, calculate and print statistics we have created some classes that will update the statistics in real-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stream_stats.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stream_stats.py\n",
    "\n",
    "class StreamStats():\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.sum = 0\n",
    "        self.squared_sum = 0\n",
    "        self.min = 1\n",
    "        self.max = 1\n",
    "        self.mean = 0.0\n",
    "        self.std = 0.0\n",
    "\n",
    "    def update(self, value):\n",
    "        self.count += 1\n",
    "\n",
    "        self.sum += value\n",
    "        self.squared_sum += value ** 2\n",
    "        self.mean = self.sum / self.count\n",
    "\n",
    "        self.min = min(self.min, value)\n",
    "        self.max = max(self.max, value)\n",
    "\n",
    "        self.std = (self.squared_sum / self.count - self.mean ** 2) ** 0.5\n",
    "\n",
    "    def clear(self):\n",
    "        self.count = 0\n",
    "        self.sum = 0\n",
    "        self.squared_sum = 0\n",
    "        self.min = 1\n",
    "        self.max = 1\n",
    "        self.mean = 0.0\n",
    "        self.std = 0.0\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"mean: {self.mean} +/- {self.std}, min: {self.min}, max: {self.max}\"\n",
    "\n",
    "# Update the stats for each year, month, day\n",
    "class StreamDateStats():\n",
    "    def __init__(self):\n",
    "        self.year = StreamStats()\n",
    "        self.month = StreamStats()\n",
    "        self.day = StreamStats()\n",
    "\n",
    "        self.year_count = 0\n",
    "        self.month_count = 0\n",
    "        self.day_count = 0\n",
    "\n",
    "        self.total_count = 0\n",
    "\n",
    "    def increase(self):\n",
    "        self.year_count += 1\n",
    "        self.month_count += 1\n",
    "        self.day_count += 1\n",
    "\n",
    "        self.total_count += 1\n",
    "\n",
    "    def reset_year(self):\n",
    "        self.year.update(self.year_count)\n",
    "        self.year_count = 0\n",
    "\n",
    "    def reset_month(self):\n",
    "        self.month.update(self.month_count)\n",
    "        self.month_count = 0\n",
    "\n",
    "    def reset_day(self):\n",
    "        self.day.update(self.day_count)\n",
    "        self.day_count = 0\n",
    "\n",
    "    def clear_day(self):\n",
    "        self.day.clear()\n",
    "        self.day_count = 0\n",
    "\n",
    "    def clear_month(self):\n",
    "        self.month.clear()\n",
    "        self.month_count = 0\n",
    "        \n",
    "class CounterDateStats():\n",
    "    def __init__(self):\n",
    "        self.counter = {}\n",
    "\n",
    "    def increase(self, key):\n",
    "        if key in [None, \"None\", \"nan\"]:\n",
    "            return\n",
    "\n",
    "        if key not in self.counter:\n",
    "            self.counter[key] = StreamDateStats()\n",
    "\n",
    "        self.counter[key].increase()\n",
    "\n",
    "    def items(self):\n",
    "        return sorted(self.counter.items(), key=lambda x: x[1].total_count, reverse=True)\n",
    "\n",
    "    def most_commons(self, n):\n",
    "        return sorted(self.counter.items(), key=lambda x: x[1].total_count, reverse=True)[:n]\n",
    "\n",
    "    def reset_year(self):\n",
    "        for key in self.counter:\n",
    "            self.counter[key].reset_year()\n",
    "\n",
    "    def reset_month(self):\n",
    "        for key in self.counter:\n",
    "            self.counter[key].reset_month()\n",
    "\n",
    "    def reset_day(self):\n",
    "        for key in self.counter:\n",
    "            self.counter[key].reset_day()\n",
    "\n",
    "    def clear_month(self):\n",
    "        for key in self.counter:\n",
    "            self.counter[key].clear_month()\n",
    "\n",
    "    def clear_day(self):\n",
    "        for key in self.counter:\n",
    "            self.counter[key].clear_day()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faust Application\n",
    "\n",
    "To run the faust application is necessary to install the requirement libraries through conda and run faust with the command \"faust -A faust_app worker -l info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting faust_app.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%6|1724158371.423|FAIL|cristian-pc#producer-3| [thrd:localhost:29092/bootstrap]: localhost:29092/1: Disconnected while requesting ApiVersion: might be caused by incorrect security.protocol configuration (connecting to a SSL listener?) or broker version is < 0.10 (see api.version.request) (after 4ms in state APIVERSION_QUERY)\n",
      "%6|1724158371.620|FAIL|cristian-pc#producer-3| [thrd:localhost:29092/bootstrap]: localhost:29092/1: Disconnected while requesting ApiVersion: might be caused by incorrect security.protocol configuration (connecting to a SSL listener?) or broker version is < 0.10 (see api.version.request) (after 0ms in state APIVERSION_QUERY, 1 identical error(s) suppressed)\n",
      "%3|1724158372.568|FAIL|cristian-pc#producer-3| [thrd:localhost:29092/bootstrap]: localhost:29092/1: Connect to ipv4#127.0.0.1:29092 failed: Connection refused (after 6ms in state CONNECT)\n",
      "%3|1724158402.424|FAIL|cristian-pc#producer-3| [thrd:localhost:29092/bootstrap]: localhost:29092/1: Connect to ipv4#127.0.0.1:29092 failed: Connection refused (after 0ms in state CONNECT, 30 identical error(s) suppressed)\n",
      "%3|1724158432.429|FAIL|cristian-pc#producer-3| [thrd:localhost:29092/bootstrap]: localhost:29092/1: Connect to ipv4#127.0.0.1:29092 failed: Connection refused (after 0ms in state CONNECT, 30 identical error(s) suppressed)\n"
     ]
    }
   ],
   "source": [
    "%%writefile faust_app.py\n",
    "\n",
    "from typing import List\n",
    "import faust\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "from parking_violations import ParkingViolation\n",
    "from stream_stats import StreamStats, StreamDateStats, CounterDateStats\n",
    "from collections import Counter, deque\n",
    "from river.cluster import DenStream\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Initialize lists to store data points and their cluster IDs\n",
    "buffer_size = 100000\n",
    "data_points = deque(maxlen=buffer_size)\n",
    "cluster_ids = deque(maxlen=buffer_size)\n",
    "\n",
    "topic = \"parking_violations\"\n",
    "\n",
    "app = faust.App(topic, broker='kafka://localhost:29092')\n",
    "print(f\"App is {app}\")\n",
    "\n",
    "violations_topic = app.topic(topic, value_type=ParkingViolation)\n",
    "\n",
    "@app.agent(violations_topic)\n",
    "async def process_violations(violations: List[ParkingViolation]):\n",
    "    # Initialize the DenStream\n",
    "    denstream = DenStream(epsilon=0.01, mu=30, beta=0.2)\n",
    "\n",
    "    all_data_stats = StreamDateStats()\n",
    "    borough_stats = CounterDateStats()\n",
    "    streets_stats = CounterDateStats()\n",
    "\n",
    "    tmp_all_data_stats = StreamDateStats()\n",
    "    tmp_borough_stats = CounterDateStats()\n",
    "    tmp_streets_stats = CounterDateStats()\n",
    "\n",
    "    current_year = None\n",
    "    current_month = None\n",
    "    current_day = None\n",
    "\n",
    "    code_to_borough = {\n",
    "        1: \"Manhattan\",\n",
    "        2: \"Bronx\",\n",
    "        3: \"Brooklyn\",\n",
    "        4: \"Queens\",\n",
    "        5: \"Staten Island\"\n",
    "    }\n",
    "\n",
    "    async for violation in violations:\n",
    "        violation.issue_date = datetime.strptime(violation.issue_date, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        if current_year is None:\n",
    "            current_year = violation.issue_date.year\n",
    "            current_month = violation.issue_date.month\n",
    "            current_day = violation.issue_date.day\n",
    "\n",
    "        # If a new day starts, reset the day\n",
    "        if violation.issue_date.day != current_day:\n",
    "            tmp_all_data_stats.reset_day()\n",
    "            tmp_borough_stats.reset_day()\n",
    "            tmp_streets_stats.reset_day()\n",
    "\n",
    "            all_data_stats.reset_day()\n",
    "            borough_stats.reset_day()\n",
    "            streets_stats.reset_day()\n",
    "\n",
    "            current_day = violation.issue_date.day\n",
    "\n",
    "        # If a new month starts, print the daily stats of the previous month (mean of violations per day) and clear the stats\n",
    "        if violation.issue_date.month != current_month:\n",
    "            tmp_all_data_stats.reset_month()\n",
    "            tmp_borough_stats.reset_month()\n",
    "            tmp_streets_stats.reset_month()\n",
    "\n",
    "            all_data_stats.reset_month()\n",
    "            borough_stats.reset_month()\n",
    "            streets_stats.reset_month()\n",
    "\n",
    "            print(f\"Daily stats of month {current_month} of year {current_year}:\")\n",
    "            print(f\"- Overall stats: total: {tmp_all_data_stats.total_count}, {tmp_all_data_stats.day}\")\n",
    "            print(\"- Borough stats:\")\n",
    "            for borough, stats in tmp_borough_stats.items():\n",
    "                borough_name = code_to_borough[int(float(borough))]\n",
    "                print(f\".      {borough_name} stats: total: {stats.total_count}, {stats.day}\")\n",
    "            print(\"- Most common streets:\")\n",
    "            for idx, street in enumerate(tmp_streets_stats.most_commons(10)):\n",
    "                print(f\".      Street {idx + 1} ({street[0]}) stats: total: {street[1].total_count}, {street[1].day}\")\n",
    "            print()\n",
    "\n",
    "            # clear the rolling stats\n",
    "            tmp_all_data_stats.clear_day()\n",
    "            tmp_borough_stats.clear_day()\n",
    "            tmp_streets_stats.clear_day()\n",
    "\n",
    "            current_month = violation.issue_date.month\n",
    "\n",
    "        # if a new year starts, print the stats of the previous year (mean of violations per month) and clear the stats\n",
    "        if violation.issue_date.year != current_year:\n",
    "            tmp_all_data_stats.reset_year()\n",
    "            tmp_borough_stats.reset_year()\n",
    "            tmp_streets_stats.reset_year()\n",
    "\n",
    "            all_data_stats.reset_year()\n",
    "            borough_stats.reset_year()\n",
    "            streets_stats.reset_year()\n",
    "\n",
    "            print(f\"Monthly stats of year {current_year}:\")\n",
    "            print(f\"- Overall stats: total: {tmp_all_data_stats.total_count}, {tmp_all_data_stats.month}\")\n",
    "            print(\"- Borough stats:\")\n",
    "            for borough, stats in tmp_borough_stats.items():\n",
    "                borough_name = code_to_borough[int(float(borough))]\n",
    "                print(f\".      {borough_name} stats: total: {stats.total_count}, {stats.month}\")\n",
    "            print(\"- Most common streets:\")\n",
    "            for idx, street in enumerate(tmp_streets_stats.most_commons(10)):\n",
    "                print(f\".      Street {idx + 1} ({street[0]}) stats: total: {street[1].total_count}, {street[1].month}\")\n",
    "            print()\n",
    "\n",
    "            # clear the rolling stats\n",
    "            tmp_all_data_stats.clear_month()\n",
    "            tmp_borough_stats.clear_month()\n",
    "            tmp_streets_stats.clear_month()\n",
    "\n",
    "            current_year = violation.issue_date.year\n",
    "\n",
    "        # print the overall stats of the data every 2000000 records\n",
    "        if all_data_stats.total_count % 2000000 == 0 and all_data_stats.total_count != 0:\n",
    "            # print overall data stats\n",
    "            print(f\"Overall stats from beginning:\")\n",
    "            print(\"- Overall stats: total: {all_data_stats.total_count}\")\n",
    "            print(f\".      yearly: {all_data_stats.year}\")\n",
    "            print(f\".      monthly: {all_data_stats.month}\")\n",
    "            print(f\".      daily: {all_data_stats.day}\")\n",
    "            print(\"- Borough stats:\")\n",
    "            for borough, stats in tmp_borough_stats.items():\n",
    "                print(f\".      {borough} stats: {stats.total_count}\")\n",
    "                print(f\".             yearly: {stats.year}\")\n",
    "                print(f\".             monthly: {stats.month}\")\n",
    "                print(f\".             daily: {stats.day}\")\n",
    "            print(\"- Most common streets:\")\n",
    "            for idx, street in enumerate(tmp_streets_stats.most_commons(10)):\n",
    "                print(f\".      Street {idx + 1} ({street[0]}) stats: total: {street[1].total_count}\")\n",
    "                print(f\".             yearly: {street[1].year}\")\n",
    "                print(f\".             monthly: {street[1].month}\")\n",
    "                print(f\".             daily: {street[1].day}\")\n",
    "            print()\n",
    "\n",
    "        # increase the stats with current violation\n",
    "        all_data_stats.increase()\n",
    "        borough_stats.increase(violation.violation_county)\n",
    "        streets_stats.increase(violation.street_code)\n",
    "\n",
    "        tmp_all_data_stats.increase()\n",
    "        tmp_borough_stats.increase(violation.violation_county)\n",
    "        tmp_streets_stats.increase(violation.street_code)\n",
    "\n",
    "        # update the denstream with the current violation\n",
    "        denstream.learn_one({\"latitude\": float(violation.latitude), \"longitude\": float(violation.longitude)})\n",
    "\n",
    "        # Store the data point and its cluster ID\n",
    "        data_points.append((float(violation.latitude), float(violation.longitude)))\n",
    "        cluster_ids.append(denstream.predict_one({\"latitude\": float(violation.latitude), \"longitude\": float(violation.longitude)}))\n",
    "\n",
    "        if all_data_stats.total_count % 200000 == 0 and all_data_stats.total_count != 0:\n",
    "            print(f\"Denstream stats: {denstream.n_clusters}\")\n",
    "\n",
    "            if denstream.n_clusters > 1:\n",
    "                # Plot the clusters\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.scatter(\n",
    "                    [point[1] for point in data_points],\n",
    "                    [point[0] for point in data_points],\n",
    "                    c=cluster_ids,\n",
    "                    cmap='viridis',\n",
    "                    marker='o',\n",
    "                    alpha=0.1,\n",
    "                    s=10\n",
    "                )\n",
    "                plt.title('DenStream Clusters')\n",
    "                plt.xlabel('Longitude')\n",
    "                plt.ylabel('Latitude')\n",
    "                plt.colorbar(label='Cluster ID')\n",
    "\n",
    "                # Save the plot as a PDF file\n",
    "                plt.savefig(f'plots/denstream_clusters_{all_data_stats.total_count}.pdf', format='pdf')\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is in chronological ordered only by issue_date, not by violation_time. So no stats on the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2014.csv\n",
      "Sending 6760 records out of 10000 (67.60%)\n",
      "Sending 14511 records out of 20000 (72.56%)\n",
      "Sending 20893 records out of 30000 (69.64%)\n",
      "Sending 25170 records out of 40000 (62.92%)\n",
      "Sending 30523 records out of 50000 (61.05%)\n",
      "Sending 36071 records out of 60000 (60.12%)\n",
      "Sending 41226 records out of 70000 (58.89%)\n",
      "Sending 45815 records out of 80000 (57.27%)\n",
      "Sending 54185 records out of 90000 (60.21%)\n",
      "Sending 61959 records out of 100000 (61.96%)\n",
      "Sending 71525 records out of 110000 (65.02%)\n",
      "Sending 79402 records out of 120000 (66.17%)\n",
      "Sending 88862 records out of 130000 (68.36%)\n",
      "Sending 96430 records out of 140000 (68.88%)\n",
      "Sending 105111 records out of 150000 (70.07%)\n",
      "Sending 113720 records out of 160000 (71.08%)\n",
      "Sending 120736 records out of 170000 (71.02%)\n",
      "Sending 130314 records out of 180000 (72.40%)\n",
      "Sending 137878 records out of 190000 (72.57%)\n",
      "Sending 146358 records out of 200000 (73.18%)\n",
      "Sending 154536 records out of 210000 (73.59%)\n",
      "Sending 162573 records out of 220000 (73.90%)\n",
      "Sending 172192 records out of 230000 (74.87%)\n",
      "Sending 179880 records out of 240000 (74.95%)\n",
      "Sending 189437 records out of 250000 (75.77%)\n",
      "Sending 197405 records out of 260000 (75.92%)\n",
      "Sending 206035 records out of 270000 (76.31%)\n",
      "Sending 212633 records out of 280000 (75.94%)\n",
      "Sending 220624 records out of 290000 (76.08%)\n",
      "Sending 228640 records out of 300000 (76.21%)\n",
      "Sending 237733 records out of 310000 (76.69%)\n",
      "Sending 245653 records out of 320000 (76.77%)\n",
      "Sending 255345 records out of 330000 (77.38%)\n",
      "Sending 262971 records out of 340000 (77.34%)\n",
      "Sending 272583 records out of 350000 (77.88%)\n",
      "Sent: 263000\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msend_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m records out of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msend_count\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mtotal_count\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, line \u001b[38;5;129;01min\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m---> 16\u001b[0m     year \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43missue_date\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39myear\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m year \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2013\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m year \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2024\u001b[39m:\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/kafka/lib/python3.11/site-packages/pandas/core/tools/datetimes.py:1101\u001b[0m, in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1099\u001b[0m         result \u001b[38;5;241m=\u001b[39m convert_listlike(argc, \u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1101\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43marg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, np\u001b[38;5;241m.\u001b[39mbool_):\n\u001b[1;32m   1103\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(result)  \u001b[38;5;66;03m# TODO: avoid this kludge.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/kafka/lib/python3.11/site-packages/pandas/core/tools/datetimes.py:429\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    426\u001b[0m arg \u001b[38;5;241m=\u001b[39m ensure_object(arg)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 429\u001b[0m     \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43m_guess_datetime_format_for_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdayfirst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/kafka/lib/python3.11/site-packages/pandas/core/tools/datetimes.py:131\u001b[0m, in \u001b[0;36m_guess_datetime_format_for_array\u001b[0;34m(arr, dayfirst)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (first_non_null \u001b[38;5;241m:=\u001b[39m tslib\u001b[38;5;241m.\u001b[39mfirst_non_null(arr)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(first_non_nan_element \u001b[38;5;241m:=\u001b[39m arr[first_non_null]) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:  \u001b[38;5;66;03m# noqa: E721\u001b[39;00m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;66;03m# GH#32264 np.str_ object\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m         guessed_format \u001b[38;5;241m=\u001b[39m \u001b[43mguess_datetime_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfirst_non_nan_element\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdayfirst\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m guessed_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m guessed_format\n",
      "File \u001b[0;32mparsing.pyx:1057\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.parsing.guess_datetime_format\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "total_count = 0\n",
    "send_count = 0\n",
    "current_count = 0\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    print(f\"Processing {csv_file}\")\n",
    "    df = pd.read_csv(base_path + \"original_data/\" + csv_files[0], chunksize=10000)\n",
    "    for chunk in df:\n",
    "        total_count += len(chunk)\n",
    "        chunk = preprocess_chunk(chunk)\n",
    "        send_count += len(chunk)\n",
    "        print(f\"Sending {send_count} records out of {total_count} ({send_count / total_count * 100:.2f}%)\")\n",
    "        for index, line in chunk.iterrows():\n",
    "            year = pd.to_datetime(line['issue_date']).year\n",
    "            if year < 2013 or year > 2024:\n",
    "                continue\n",
    "\n",
    "            record_key = str(uuid.uuid4())\n",
    "            record_value = line.to_dict()\n",
    "            producer.produce(topic, key=record_key, value=json.dumps(record_value))\n",
    "            producer.poll(0)\n",
    "            current_count += 1\n",
    "            # update printing\n",
    "            if current_count % 1000 == 0:\n",
    "                print(f\"Sent: {current_count}\", end=\"\\r\")\n",
    "                time.sleep(0.1)\n",
    "        \n",
    "    del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer.flush()\n",
    "consumer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bd39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
